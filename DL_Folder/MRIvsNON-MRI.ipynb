{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bVfm4uf9s1ws",
        "outputId": "cd2d8e6c-9e72-4a1a-ac65-360cb50b08c4"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H4y1c0Nm8C38",
        "outputId": "908a498d-080f-4177-8453-ca4421bfd1dc"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import glob\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "\n",
        "# Define Paths\n",
        "dataset_dir = \"/content/drive/MyDrive/MRI AND NON MRI\"  # Root folder containing class folders\n",
        "# Adjust class names to match actual folder names\n",
        "classes = [\"Brain_MRI\",\"Non_Brain_MRI\"]\n",
        "\n",
        "# Load Data with Correct Folder Names\n",
        "image_paths, labels = [], []\n",
        "for class_label, class_name in enumerate(classes):\n",
        "    class_dir = os.path.join(dataset_dir, class_name)\n",
        "    if not os.path.exists(class_dir):\n",
        "        print(f\"Error: Folder {class_dir} does not exist.\")\n",
        "        continue\n",
        "    files = glob.glob(f\"{class_dir}/*.[jp][pn]g\")\n",
        "  # Adjust extension if needed\n",
        "    print(f\"Class: {class_name}, Files Found: {len(files)}\")  # Debug: Count files\n",
        "    for file_path in files:\n",
        "        image_paths.append(file_path)\n",
        "        labels.append(class_label)\n",
        "\n",
        "# Proceed with the pipeline if files are found\n",
        "if len(image_paths) == 0:\n",
        "    raise ValueError(\"No images found. Check dataset folder names or file paths.\")\n",
        "\n",
        "# Split Dataset\n",
        "train_paths, test_paths, train_labels, test_labels = train_test_split(\n",
        "    image_paths, labels, test_size=0.2, random_state=42, stratify=labels\n",
        ")\n",
        "\n",
        "print(f\"Total images: {len(image_paths)}\")\n",
        "print(f\"Training images: {len(train_paths)}, Testing images: {len(test_paths)}\")\n",
        "\n",
        "# One-hot encode the labels using to_categorical\n",
        "# train_labels = tf.keras.utils.to_categorical(train_labels, num_classes=len(classes))\n",
        "# test_labels = tf.keras.utils.to_categorical(test_labels, num_classes=len(classes))\n",
        "\n",
        "# The previous one-hot encoding method was:\n",
        "# label_binarizer = LabelBinarizer()\n",
        "# train_labels = label_binarizer.fit_transform(train_labels)\n",
        "# test_labels = label_binarizer.transform(test_labels)\n",
        "\n",
        "# Normalize image paths and labels\n",
        "train_paths = np.array(train_paths)\n",
        "test_paths = np.array(test_paths)\n",
        "\n",
        "# Preprocessing Function for Images\n",
        "IMG_HEIGHT, IMG_WIDTH = 224, 224\n",
        "\n",
        "def preprocess_image(image_path):\n",
        "    image = tf.io.read_file(image_path)\n",
        "    image = tf.image.decode_jpeg(image, channels=3)\n",
        "    image = tf.image.resize(image, [IMG_HEIGHT, IMG_WIDTH])\n",
        "    image = image / 255.0  # Normalize to [0, 1]\n",
        "    return image\n",
        "\n",
        "# Load Dataset\n",
        "def load_dataset(image_paths, labels):\n",
        "    # Convert labels to one-hot encoding before creating dataset\n",
        "    labels = tf.keras.utils.to_categorical(labels, num_classes=len(classes))\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((image_paths, labels))\n",
        "    dataset = dataset.map(lambda x, y: (preprocess_image(x), y))\n",
        "    return dataset\n",
        "\n",
        "# Prepare Train and Test Datasets\n",
        "train_dataset = load_dataset(train_paths, train_labels)\n",
        "test_dataset = load_dataset(test_paths, test_labels)\n",
        "\n",
        "# Batch and Shuffle the Datasets\n",
        "BATCH_SIZE = 32\n",
        "train_dataset = train_dataset.shuffle(buffer_size=1000).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
        "test_dataset = test_dataset.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
        "for images, labels in train_dataset.take(1):\n",
        "    print(\"Images shape:\", images.shape)\n",
        "    print(\"Labels shape:\", labels.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 504
        },
        "id": "JBksrlFl7xyf",
        "outputId": "f8d84ba3-deb8-4a61-ed14-85f6fa9c8e23"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras import layers, models\n",
        "\n",
        "def build_model():\n",
        "    model = models.Sequential([\n",
        "        layers.Conv2D(32, (3, 3), activation='relu', input_shape=(IMG_HEIGHT, IMG_WIDTH, 3)),\n",
        "        layers.MaxPooling2D((2, 2)),\n",
        "        layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "        layers.MaxPooling2D((2, 2)),\n",
        "        layers.Conv2D(128, (3, 3), activation='relu'),\n",
        "        layers.MaxPooling2D((2, 2)),\n",
        "        layers.Flatten(),\n",
        "        layers.Dense(128, activation='relu'),\n",
        "        layers.Dropout(0.5),\n",
        "        layers.Dense(len(classes), activation='softmax')  # Number of classes\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "model = build_model()\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T39oppX19EDB",
        "outputId": "e57ec4e8-5e50-4cf2-8aa8-becb55c45362"
      },
      "outputs": [],
      "source": [
        " # Compile the Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "model.compile(optimizer=Adam(learning_rate=0.001),\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train the Model\n",
        "EPOCHS = 10\n",
        "history = model.fit(\n",
        "    train_dataset,\n",
        "    validation_data=test_dataset,\n",
        "    epochs=EPOCHS\n",
        ")\n",
        "\n",
        "# Save the Model\n",
        "model.save(\"alzheimers_detection_model.h5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 375
        },
        "id": "3dNkwxnv8Gsd",
        "outputId": "213a6db4-6ceb-43e4-e06d-527fec61736f"
      },
      "outputs": [],
      "source": [
        " # Evaluate the Model\n",
        "loss, accuracy = model.evaluate(test_dataset)\n",
        "print(f\"Test Loss: {loss:.4f}, Test Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Plot Accuracy and Loss\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_history(history):\n",
        "    plt.figure(figsize=(12, 4))\n",
        "    # Accuracy Plot\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "    plt.legend()\n",
        "    plt.title('Accuracy')\n",
        "    # Loss Plot\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(history.history['loss'], label='Training Loss')\n",
        "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "    plt.legend()\n",
        "    plt.title('Loss')\n",
        "    plt.show()\n",
        "\n",
        "plot_history(history)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 500
        },
        "id": "NSa05rh3QzZr",
        "outputId": "8397e915-f15e-43c7-9638-93350661dc0d"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "\n",
        "# Load the Saved Model\n",
        "loaded_model = load_model(\"alzheimers_detection_model.h5\")\n",
        "\n",
        "# Define the classes again\n",
        "classes = [\"Brain_MRI\", \"Non_Brain_MRI\"]\n",
        "\n",
        "# Testing on a Single Image\n",
        "test_image_path = '/content/drive/MyDrive/MRI AND NON MRI/Brain_MRI/Te-pi_0285.jpg'  # Replace with actual image path\n",
        "\n",
        "# Preprocess the image\n",
        "test_image = preprocess_image(test_image_path)\n",
        "test_image = tf.expand_dims(test_image, axis=0)  # Adding batch dimension\n",
        "\n",
        "# Predict the class\n",
        "prediction = loaded_model.predict(test_image)\n",
        "\n",
        "# Get the predicted class\n",
        "predicted_class_idx = np.argmax(prediction)  # Get index of the max value\n",
        "predicted_class = classes[predicted_class_idx]  # Map index to class name\n",
        "print(f\"Predicted Class: {predicted_class}\")\n",
        "\n",
        "# Display Image + Prediction\n",
        "plt.imshow(plt.imread(test_image_path))\n",
        "plt.title(f\"Predicted Class: {predicted_class}\")\n",
        "plt.axis('off')\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
